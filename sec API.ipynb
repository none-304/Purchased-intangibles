{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#api key\n",
    "API_KEY ='0d02265676b10a2702f45b78f36b0e1b0840bb05eb4c436ecf217440c2b80254'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate XBRL api\n",
    "import pandas as pd\n",
    "from sec_api import XbrlApi\n",
    "\n",
    "xbrlApi = XbrlApi(API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate Query api\n",
    "from sec_api import QueryApi\n",
    "\n",
    "queryApi = QueryApi(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab filing URL\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def standardize_filing_url(url):\n",
    "  return url.replace('ix?doc=/', '')\n",
    "\n",
    "\n",
    "def get_10K_metadata(start_year = 2021, end_year = 2022):\n",
    "  frames = []\n",
    "\n",
    "  for year in range(start_year, end_year + 1):\n",
    "    number_of_objects_downloaded = 0\n",
    "\n",
    "    for month in range(1, 13):\n",
    "      padded_month = str(month).zfill(2) # \"1\" -> \"01\"\n",
    "      date_range_filter = f'filedAt:[{year}-{padded_month}-01 TO {year}-{padded_month}-31]'\n",
    "      form_type_filter  = f'formType:\"10-K\" AND NOT formType:(\"10-K/A\", NT)'\n",
    "      lucene_query = date_range_filter + ' AND ' + form_type_filter\n",
    "\n",
    "      query_from = 0\n",
    "      query_size = 200\n",
    "\n",
    "      while True:\n",
    "        query = {\n",
    "          \"query\": lucene_query,\n",
    "          \"from\": query_from,\n",
    "          \"size\": query_size,\n",
    "          \"sort\": [{ \"filedAt\": { \"order\": \"desc\" } }]\n",
    "        }\n",
    "\n",
    "        response = queryApi.get_filings(query)\n",
    "        filings = response['filings']\n",
    "\n",
    "        if len(filings) == 0:\n",
    "          break\n",
    "        else:\n",
    "          query_from += query_size\n",
    "\n",
    "        metadata = list(map(lambda f: {'ticker': f['ticker'], \n",
    "                                       'cik': f['cik'], \n",
    "                                       'formType': f['formType'], \n",
    "                                       'filedAt': f['filedAt'], \n",
    "                                       'filingUrl': f['linkToFilingDetails']\n",
    "                                      }, filings))\n",
    "\n",
    "        df = pd.DataFrame.from_records(metadata)\n",
    "        # remove all entries without a ticker symbol\n",
    "        df = df[df['ticker'].str.len() > 0]\n",
    "        df['filingUrl'] = df['filingUrl'].apply(standardize_filing_url)\n",
    "        frames.append(df)\n",
    "        number_of_objects_downloaded += len(df)\n",
    "\n",
    "    print(f'✅ Downloaded {number_of_objects_downloaded} metadata objects for year {year}')\n",
    "\n",
    "  result = pd.concat(frames)\n",
    "\n",
    "  print(f'✅ Download completed. Metadata downloaded for {len(result)} filings.')\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Downloaded 5695 metadata objects for year 2017\n",
      "✅ Downloaded 5683 metadata objects for year 2018\n",
      "✅ Download completed. Metadata downloaded for 11378 filings.\n"
     ]
    }
   ],
   "source": [
    "#download metadata\n",
    "metadata_10K = get_10K_metadata(start_year=2017, end_year=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save metadata\n",
    "metadata_10K.to_pickle('metadata_10K 2017_2018.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split download per year\n",
    "import pandas as pd\n",
    "year='2021'\n",
    "data=pd.read_pickle(f'metadata_10K 2021_2022.pkl')\n",
    "data[data.filedAt.astype(str).str[:4]==year].to_pickle(f'json {year}.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5997/5997 [13:42<00:00,  7.29it/s]  \n"
     ]
    }
   ],
   "source": [
    "#fetch XBRL\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "def fetch_xbrl_json(url):\n",
    "    try:\n",
    "        xbrl_json = xbrlApi.xbrl_to_json(htm_url=url)\n",
    "        return xbrl_json\n",
    "    except:\n",
    "        return np.nan\n",
    "json_list = []\n",
    "\n",
    "\n",
    "data=pd.read_pickle('json 2021.pkl').reset_index()\n",
    "\n",
    "\n",
    "# Using ThreadPoolExecutor for concurrent processing\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(fetch_xbrl_json, row.filingUrl) for index, row in data.iterrows()]\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "        json_list.append(future.result())\n",
    "data['json'] = json_list\n",
    "data.to_pickle('json_fetched_2021.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>ticker</th>\n",
       "      <th>cik</th>\n",
       "      <th>formType</th>\n",
       "      <th>filedAt</th>\n",
       "      <th>filingUrl</th>\n",
       "      <th>json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>AMD</td>\n",
       "      <td>2488</td>\n",
       "      <td>10-K</td>\n",
       "      <td>2021-01-29T17:24:00-05:00</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/2488/0...</td>\n",
       "      <td>{'CoverPage': {'EntityCentralIndexKey': '00015...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>ASTI</td>\n",
       "      <td>1350102</td>\n",
       "      <td>10-K</td>\n",
       "      <td>2021-01-29T16:02:57-05:00</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/135010...</td>\n",
       "      <td>{'CoverPage': {'DocumentType': '10-K', 'Docume...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>DUSYF</td>\n",
       "      <td>1551887</td>\n",
       "      <td>10-K</td>\n",
       "      <td>2021-01-29T15:17:44-05:00</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/155188...</td>\n",
       "      <td>{'CoverPage': {'EntityRegistrantName': 'Digipa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>KSU</td>\n",
       "      <td>54480</td>\n",
       "      <td>10-K</td>\n",
       "      <td>2021-01-29T13:52:54-05:00</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/54480/...</td>\n",
       "      <td>{'CoverPage': {'EntityCentralIndexKey': '00002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>FREVS</td>\n",
       "      <td>36840</td>\n",
       "      <td>10-K</td>\n",
       "      <td>2021-01-29T13:29:05-05:00</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/36840/...</td>\n",
       "      <td>{'CoverPage': {'DocumentType': '10-K', 'Docume...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index ticker      cik formType                    filedAt  \\\n",
       "0      0    AMD     2488     10-K  2021-01-29T17:24:00-05:00   \n",
       "1      2   ASTI  1350102     10-K  2021-01-29T16:02:57-05:00   \n",
       "2      3  DUSYF  1551887     10-K  2021-01-29T15:17:44-05:00   \n",
       "3      5    KSU    54480     10-K  2021-01-29T13:52:54-05:00   \n",
       "4      6  FREVS    36840     10-K  2021-01-29T13:29:05-05:00   \n",
       "\n",
       "                                           filingUrl  \\\n",
       "0  https://www.sec.gov/Archives/edgar/data/2488/0...   \n",
       "1  https://www.sec.gov/Archives/edgar/data/135010...   \n",
       "2  https://www.sec.gov/Archives/edgar/data/155188...   \n",
       "3  https://www.sec.gov/Archives/edgar/data/54480/...   \n",
       "4  https://www.sec.gov/Archives/edgar/data/36840/...   \n",
       "\n",
       "                                                json  \n",
       "0  {'CoverPage': {'EntityCentralIndexKey': '00015...  \n",
       "1  {'CoverPage': {'DocumentType': '10-K', 'Docume...  \n",
       "2  {'CoverPage': {'EntityRegistrantName': 'Digipa...  \n",
       "3  {'CoverPage': {'EntityCentralIndexKey': '00002...  \n",
       "4  {'CoverPage': {'DocumentType': '10-K', 'Docume...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 1596/5695 [03:55<2:25:02,  2.12s/it]IOStream.flush timed out\n",
      "100%|██████████| 5695/5695 [13:30<00:00,  7.03it/s]  \n",
      "100%|██████████| 5683/5683 [13:34<00:00,  6.98it/s]  \n",
      "100%|██████████| 5627/5627 [14:05<00:00,  6.66it/s]  \n",
      "100%|██████████| 5526/5526 [13:14<00:00,  6.95it/s]  \n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#loop for xbrl fetch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "# Function to fetch xbrl_json\n",
    "def fetch_xbrl_json(url):\n",
    "    try:\n",
    "        xbrl_json = xbrlApi.xbrl_to_json(htm_url=url)\n",
    "        return xbrl_json\n",
    "    except:\n",
    "        return np.nan\n",
    "for year in np.arange(2017,2023):\n",
    "\n",
    "    # List to store JSON results\n",
    "    data=pd.read_pickle(f'json {year}.pkl').reset_index()\n",
    "    json_list = []\n",
    "\n",
    "    # Using ThreadPoolExecutor for concurrent processing\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(fetch_xbrl_json, row.filingUrl) for index, row in data.iterrows()]\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "            json_list.append(future.result())\n",
    "\n",
    "    data['json'] = json_list\n",
    "    data.to_pickle(f'json_fetched_{year}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [04:48<00:00, 288.35s/it]\n"
     ]
    }
   ],
   "source": [
    "#loop for partial tag match\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def check_for_partial_matches(df, key_to_check, delimiter=';'):\n",
    "    def find_partial_matches(json_data, key_to_check, parent_key=None):\n",
    "        matches = set()\n",
    "        if isinstance(json_data, dict):\n",
    "            for key, value in json_data.items():\n",
    "                current_key = key if parent_key is None else f\"{parent_key}.{key}\"\n",
    "                if key_to_check.lower() in key.lower():\n",
    "                    matches.add(parent_key if parent_key else key)\n",
    "                if isinstance(value, dict):\n",
    "                    matches.update(find_partial_matches(value, key_to_check, current_key))\n",
    "                elif isinstance(value, list):\n",
    "                    for item in value:\n",
    "                        if isinstance(item, dict):\n",
    "                            matches.update(find_partial_matches(item, key_to_check, current_key))\n",
    "        return matches\n",
    "\n",
    "    col_name_match = f'contains_{key_to_check.lower()}'\n",
    "    col_name_key = f'key_contains_{key_to_check.lower()}'\n",
    "\n",
    "    # Apply the function to each row and collect matches\n",
    "    df[col_name_key] = df['json'].apply(lambda x: delimiter.join(find_partial_matches(x, key_to_check)))\n",
    "    df[col_name_match] = df[col_name_key].apply(lambda x: len(x) > 0)\n",
    "\n",
    "    return df\n",
    "key_to_check = 'BusinessCombinationRecognizedIdentifiableAssets'\n",
    "\n",
    "\n",
    "for year in tqdm(np.arange(2021,2022)):\n",
    "    data=pd.read_pickle(f'json_fetched_{year}.pkl')\n",
    "# Check for partial matches in the json column\n",
    "    result_partial_matches_df = check_for_partial_matches(data, key_to_check)\n",
    "    result_partial_matches_df[result_partial_matches_df['contains_businesscombinationrecognizedidentifiableassets']==True].to_pickle(f'checked partial/partial_check{year}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1645it [00:06, 261.65it/s]\n"
     ]
    }
   ],
   "source": [
    "#loop for expanding the content of the tag\n",
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to flatten the DataFrame\n",
    "def flatten_dataframe(df):\n",
    "    rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        details = row['details']\n",
    "        if isinstance(details, list):\n",
    "            for item in details:\n",
    "                if isinstance(item, dict):\n",
    "                    new_row = {**row.to_dict(), **item}\n",
    "                else:\n",
    "                    new_row = {**row.to_dict(), 'details': item}\n",
    "                rows.append(new_row)\n",
    "        elif isinstance(details, dict):\n",
    "            new_row = {**row.to_dict(), **details}\n",
    "            rows.append(new_row)\n",
    "        else:\n",
    "            new_row = row.to_dict()\n",
    "            rows.append(new_row)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Function to process segments\n",
    "def process_segments(segment):\n",
    "    try:\n",
    "        flattened_segment = json_normalize(segment)\n",
    "        combined_values = flattened_segment.apply(lambda x: ';'.join(x.astype(str)), axis=1)\n",
    "        return ';'.join(combined_values)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "\n",
    "for year in np.arange(2021,2022):\n",
    "    all_expanded_dfs = []\n",
    "    input=pd.read_pickle(f'/Users/jonathantanone/Purchased-intangibles/checked partial/partial_check{year}.pkl')\n",
    "    # Loop through each row of the DataFrame\n",
    "    for idx, row in tqdm(input.iterrows()):\n",
    "        try:\n",
    "            # Extract the input data from the JSON column\n",
    "            input_data = row['json'][row['key_contains_businesscombinationrecognizedidentifiableassets']]\n",
    "            \n",
    "            # Create a DataFrame from the input data\n",
    "            df = pd.DataFrame(input_data.items(), columns=['category', 'details'])\n",
    "            \n",
    "            # Flatten the DataFrame\n",
    "            flattened_df = flatten_dataframe(df)\n",
    "            \n",
    "            # Remove prefixes from the 'category' column\n",
    "            prefixes = [\n",
    "                'BusinessCombinationRecognizedIdentifiableAssetsAcquiredAndLiabilitiesAssumed',\n",
    "                'BusinessCombinationRecognizedIdentifiable',\n",
    "                'BusinessCombinationIdentifiable',\n",
    "                'BusinessCombinationRecognized',\n",
    "                'BusinessCombination',\n",
    "            ]\n",
    "            for prefix in prefixes:\n",
    "                flattened_df['category'] = flattened_df['category'].str.replace(f'^{prefix}', '', regex=True)\n",
    "            try:\n",
    "                # Process segments and add the results to the DataFrame\n",
    "                flattened_df['segment_result'] = flattened_df['segment'].apply(process_segments)\n",
    "                \n",
    "                # Split the 'segment_result' column into multiple columns\n",
    "                segment_columns = flattened_df['segment_result'].str.split(';', expand=True)\n",
    "                segment_columns = segment_columns.rename(lambda x: f'segment_{x+1}', axis=1)\n",
    "                \n",
    "                # Combine the new columns with the original DataFrame\n",
    "                expanded_df = pd.concat([flattened_df, segment_columns], axis=1)\n",
    "                \n",
    "                # Drop the original 'segment_result' column if it's no longer needed\n",
    "                expanded_df = expanded_df.drop(columns=['segment_result'])\n",
    "            except:\n",
    "                expanded_df = flattened_df\n",
    "            # Append the expanded DataFrame to the list\n",
    "            all_expanded_dfs.append(expanded_df)\n",
    "        except:\n",
    "            all_expanded_dfs.append(pd.DataFrame(columns=['check']))\n",
    "\n",
    "    # Concatenate all the expanded DataFrames\n",
    "    input['table'] = all_expanded_dfs\n",
    "    input.iloc[:,1:].reset_index().iloc[:,1:].to_pickle(f'PPA tabulated/data_{year}.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop for expanding the content\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def expand_table_skip_empty(df):\n",
    "    expanded_df_list = []\n",
    "    for index, row in df.iterrows():\n",
    "        table_df = row['table']\n",
    "        if not table_df.empty:\n",
    "            table_df['ticker'] = row['ticker']\n",
    "            table_df['cik'] = row['cik']\n",
    "            table_df['filedAt'] = row['filedAt']\n",
    "            expanded_df_list.append(table_df)\n",
    "    \n",
    "    if expanded_df_list:\n",
    "        expanded_df = pd.concat(expanded_df_list, ignore_index=True)\n",
    "    else:\n",
    "        expanded_df = pd.DataFrame()  # Return an empty DataFrame if all tables are empty\n",
    "    return expanded_df\n",
    "\n",
    "for a in np.arange(2021,2022):\n",
    "    data=pd.read_pickle(f'PPA tabulated/data_{a}.pkl')\n",
    "    expanded_df=expand_table_skip_empty(data)\n",
    "    expanded_df.to_csv(f'expanded_df/ppa_expanded_{a}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/00/q80vtjnd0bv_yp6wck36g7w40000gn/T/ipykernel_4269/3514968346.py:1: DtypeWarning: Columns (21,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data=pd.read_csv('expanded_df/ppa_expanded_2017.csv')\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv('expanded_df/ppa_expanded_2017.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
